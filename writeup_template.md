[//]: # (Image References)

[image1]: ./misc/rover_image.jpg
[image2]: ./calibration_images/example_grid1.jpg
[image3]: ./calibration_images/example_rock1.jpg
[video1]: ./output/test_mapping.mp4 "Video1"
[video2]: ./output/test_mapping.mp4 "Video2"

## Project: Search and Sample Return
---
**The goals / steps of this project are the following:**  

**Training / Calibration**  

The overall approach is described in code/Rover_Project_Test_Notebook.ipynb. 

* Add functions to detect obstacles and samples of interest (golden rocks)

In order to identify navigatable terrain, obstacles, and samples of interest I created analyze_color(img, high, low, color=0, channel=0) (utility function) that helps to try various color channels and thresholds. The function is located in code cell 6 of the ipython book.
Next, I identified the best color channels and the best thresholds for:
* navigable terrain (code cell 7): Navigatable terrain is best identified in YUV color scheme on Y channel. For the threshold I used the following bounds (160, 255).
* sample rock (code cell 8): Navigatable terrain is best identified in LAB color scheme on B channel. For the threshold I used the following bounds (150, 255).

obstacles will be defined as everything else. For illustration please refere to code/perception.py lines 125 and 126.

* Fill in the `process_image()` function with the appropriate image processing steps (perspective transform, color threshold etc.) to get from raw images to a map.  The `output_image` you create in this step should demonstrate that your mapping pipeline works.

Updated process_image() function located in code cell 12 of ipython book. In order to process image correctly I added the following steps:
1) Apply color threshold to identify navigable and a rock
2) Apply perspective transform and remove values hallows generated by perspective transform
3) Convert thresholded image pixel values to rover-centric coords
4) Convert rover-centric pixel values to world coords
5) Update worldmap (to be displayed on right side of screen)

In addition, I added information related to rock sample on the resulting image. This task was accomplished by identifying sample rock and calculating the distance and the angle to the rock using detect_rock() function (in code cell 10).

* Use `moviepy` to process the images in your saved dataset with the `process_image()` function.  Include the video you produce as part of your submission.

The result of processing provided images with moviepy presented below:
[link to my video result](./output/test_mapping.mp4)



**Autonomous Navigation / Mapping**

* Fill in the `perception_step()` function within the `perception.py` script with the appropriate image processing functions to create a map and update `Rover()` data (similar to what you did with `process_image()` in the notebook).

Updated perception_step() function is located in code/perception.py lines 108 - 152.

Main changes:
1) Detect navigable terrain - achieved through a dedicated function detect_navigable_terrain(R.img)
2) Detect rock sample - achieved through a dedicated function detect_rock(R.img, min_points_to_detect)
3) Visualize results - updated Rover.vision_image in lines 122 - 129
4) Convert rover-centric pixel values to world coordinates - lines 131 - 140
5) Update world map - lines 141 - 146

* Fill in the `decision_step()` function within the `decision.py` script with conditional statements that take into consideration the outputs of the `perception_step()` in deciding how to issue throttle, brake and steering commands.

In order to achieve targeted at least 40% of the environment with 60% fidelity I made the following changes:
1) Added Rover.est_steering() method - designed to provide average angle of the widest terrain area. Average steering angle provides a good starting point for picking a direction. However, this approach is not a good fit when Rover encounters 2 paths. The solution proposed in this method splits terrain into 2 parts and chooses the wides path. For more details please refere to drive_rover.py lines 80 - 98.
2) Applied Rover.est_steering() to steering angle calculation when Rover is moving forward - descision.py line 29.

In addition, I increased maximum speed and acceleration (lines 65 and 56 respectively) to speed up the data collection.

* Iterate on your perception and decision function until your rover does a reasonable (need to define metric) job of navigating and mapping.  

As a result, Rover achieved approximately 60% coverage at 65% fidelity rate. A video illustraton is located below:
[link to my video result](./output/autonomous.mp4)


## [Rubric](https://review.udacity.com/#!/rubrics/916/view) Points
### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  

---
README

### Notebook Analysis
#### 1. Run the functions provided in the notebook on test images (first with the test data provided, next on data you have recorded). Add/modify functions to allow for color selection of obstacles and rock samples.
Here is an example of how to include an image in your writeup.

![alt text][image1]

#### 1. Populate the `process_image()` function with the appropriate analysis steps to map pixels identifying navigable terrain, obstacles and rock samples into a worldmap.  Run `process_image()` on your test data using the `moviepy` functions provided to create video output of your result. 
And another! 

![alt text][image2]
### Autonomous Navigation and Mapping

#### 1. Fill in the `perception_step()` (at the bottom of the `perception.py` script) and `decision_step()` (in `decision.py`) functions in the autonomous mapping scripts and an explanation is provided in the writeup of how and why these functions were modified as they were.


#### 2. Launching in autonomous mode your rover can navigate and map autonomously.  Explain your results and how you might improve them in your writeup.  

**Note: running the simulator with different choices of resolution and graphics quality may produce different results, particularly on different machines!  Make a note of your simulator settings (resolution and graphics quality set on launch) and frames per second (FPS output to terminal by `drive_rover.py`) in your writeup when you submit the project so your reviewer can reproduce your results.**

Here I'll talk about the approach I took, what techniques I used, what worked and why, where the pipeline might fail and how I might improve it if I were going to pursue this project further.  



![alt text][image3]


